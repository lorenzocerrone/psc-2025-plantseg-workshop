{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from plotting import plot_comparison, plot_image, plot_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./ps_tasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Setup workflow Tracing\n",
    "\n",
    "- Running the following cell will make sure that that the current workflow is \"empty\" and ready to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantseg.tasks.workflow_handler import workflow_handler\n",
    "\n",
    "workflow_handler.clean_dag()\n",
    "\n",
    "print(f\"Run-Inputs: {workflow_handler.dag.inputs}\")\n",
    "print(f\"Tasks: {workflow_handler.dag.list_tasks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import the input data\n",
    "\n",
    "- First, we need to import the input data. This is done by running the following cell.\n",
    "- Try to run the following cells with diffent inputs from the given test data (eg. [ovule_2d](https://drive.google.com/file/d/1Mfg3q-5Rj_oxLPUaqyTiqvVk9uKRB8xa/view?usp=sharing))\n",
    "- Be careful that some parementers like the `stack_layout` might need to be adjusted based on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantseg.tasks.io_tasks import import_image_task\n",
    "\n",
    "path = Path(\"PATH to your File\")\n",
    "\n",
    "ps_image = import_image_task(input_path=path, semantic_type='raw', stack_layout=\"YX\", key=\"raw\")\n",
    "\n",
    "plot_image(ps_image.get_data(), title=\"Raw Image\")\n",
    "\n",
    "print(f\"Run-Inputs: {workflow_handler.dag.inputs}\")\n",
    "print(f\"Tasks: {workflow_handler.dag.list_tasks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Run a basic workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantseg.io.voxelsize import VoxelSize\n",
    "from plantseg.tasks.dataprocessing_tasks import gaussian_smoothing_task, image_rescale_to_voxel_size_task, set_biggest_instance_to_zero_task\n",
    "from plantseg.tasks.prediction_tasks import unet_prediction_task\n",
    "from plantseg.tasks.segmentation_tasks import dt_watershed_task, clustering_segmentation_task\n",
    "\n",
    "\n",
    "smooth_image = gaussian_smoothing_task(image=ps_image, sigma=6.0)\n",
    "\n",
    "plot_comparison(ps_image.get_data(), smooth_image.get_data(), title1=\"Raw Image\", title2=\"Smooth Image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Export image and save the workflow\n",
    "\n",
    "- Runnning the following cell will export the image of the workflow and save the workflow file in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantseg.tasks.io_tasks import export_image_task\n",
    "\n",
    "export_image_task(image=smooth_image, export_directory=\"./\", name_pattern='{file_name}_export')\n",
    "\n",
    "workflow_handler.save_to_yaml(path=\"./base_workflow.yaml\")\n",
    "\n",
    "print(f\"Run-Inputs: {workflow_handler.dag.inputs}\")\n",
    "print(\"Tasks: \")\n",
    "for task in workflow_handler.dag.list_tasks:\n",
    "    print(f'- {task}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Run the workflow from the command line\n",
    "\n",
    "- Run the following command in the terminal to run the workflow from the command line.\n",
    "```bash\n",
    "plantseg --config base_workflow.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Create a more complex workflow\n",
    "\n",
    "- You can create a more complex workflow by adding more steps to the workflow.\n",
    "- Try for example to use the `unet_prediction_tak` and the `dt_watershed_task` to segment one of the test images.\n",
    "- To check the possible kewords arguments for each task you can see our [api documentation](https://kreshuklab.github.io/plant-seg/chapters/python_api/tasks/io_tasks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantseg.io.voxelsize import VoxelSize\n",
    "from plantseg.tasks.dataprocessing_tasks import gaussian_smoothing_task, image_rescale_to_voxel_size_task, set_biggest_instance_to_zero_task\n",
    "from plantseg.tasks.prediction_tasks import unet_prediction_task\n",
    "from plantseg.tasks.segmentation_tasks import dt_watershed_task, clustering_segmentation_task\n",
    "from plantseg.core.zoo import model_zoo\n",
    "\n",
    "# Print available models\n",
    "model_zoo.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantseg.tasks.workflow_handler import workflow_handler\n",
    "\n",
    "workflow_handler.clean_dag()\n",
    "\n",
    "print(f\"Run-Inputs: {workflow_handler.dag.inputs}\")\n",
    "print(f\"Tasks: {workflow_handler.dag.list_tasks}\")\n",
    "\n",
    "\n",
    "##############################################\n",
    "#\n",
    "# Place your code here\n",
    "#\n",
    "##############################################\n",
    "\n",
    "\n",
    "workflow_handler.save_to_yaml(path=\"./full_workflow.yaml\")\n",
    "\n",
    "print(f\"Run-Inputs: {workflow_handler.dag.inputs}\")\n",
    "print(\"Tasks: \")\n",
    "for task in workflow_handler.dag.list_tasks:\n",
    "    print(f'- {task}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Use plantseg functionals to interact with other libraries\n",
    "\n",
    "- You can use the plantseg functionals to interact with other libraries like napari, scikit-image, etc.\n",
    "- Plantseg functionals simply work with numpy arrays\n",
    "- Replace the `random_image` with an image from the test data and try to use the functionals to segment the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari \n",
    "# Start a napari viewer\n",
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantseg.functionals.prediction import unet_prediction\n",
    "from plantseg.functionals.dataprocessing import normalize_01, image_gaussian_smoothing, image_rescale\n",
    "from plantseg.functionals.segmentation import dt_watershed, gasp, multicut, mutex_ws\n",
    "from plantseg.io import smart_load\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "random_image = np.random.rand(512, 512)\n",
    "# test_image = smart_load(Path(\"Path to your image\"))\n",
    "\n",
    "prediction = unet_prediction(raw=random_image, \n",
    "                             input_layout=\"YX\",\n",
    "                             model_name=\"confocal_2D_unet_ovules_ds2x\",\n",
    "                             # patch=(1, 64, 64),\n",
    "                             #patch_halo=(0, 0, 0),\n",
    "                             model_id=None, device=\"mps\")\n",
    "\n",
    "# you can play with the image contrast and you can see the effect of\n",
    "# patch and patch_halo on the result\n",
    "viewer.add_image(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plant-seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
